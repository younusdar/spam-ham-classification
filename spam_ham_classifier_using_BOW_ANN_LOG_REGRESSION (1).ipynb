{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "spam_ham_classifier_using BOW_ANN_LOG_REGRESSION.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YkTjX632jZ1Y",
        "outputId": "cde1929d-84c5-40e0-9610-6966e6c49e50"
      },
      "source": [
        "#Load the libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "import nltk\n",
        "from bs4 import BeautifulSoup\n",
        "import re,string\n",
        "\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-rSvcc7jbMG",
        "outputId": "a682a878-a3d5-41dd-d492-2fd51106f667"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRLAvZDGjfdN"
      },
      "source": [
        "emails=pd.read_csv(\"/content/gdrive/MyDrive/emails.csv\")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "j-oUK9MekIdY",
        "outputId": "d13300dd-21cc-455e-ec1f-f14b915e928e"
      },
      "source": [
        "emails.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>spam</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Subject: naturally irresistible your corporate...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Subject: the stock trading gunslinger  fanny i...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Subject: unbelievable new homes made easy  im ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Subject: 4 color printing special  request add...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Subject: do not have money , get software cds ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  spam\n",
              "0  Subject: naturally irresistible your corporate...     1\n",
              "1  Subject: the stock trading gunslinger  fanny i...     1\n",
              "2  Subject: unbelievable new homes made easy  im ...     1\n",
              "3  Subject: 4 color printing special  request add...     1\n",
              "4  Subject: do not have money , get software cds ...     1"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bdqggKkkSZk"
      },
      "source": [
        "#Collating all functions together and applying them for the 'IMDb reviews' dataset\n",
        "def strip_html(text):\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    return soup.get_text()\n",
        "\n",
        "#Removing emojis\n",
        "def deEmojify(text):\n",
        "    regrex_pattern = re.compile(pattern = \"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols                                                                         \n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           \"]+\", flags = re.UNICODE)\n",
        "    return regrex_pattern.sub(r'',text)\n",
        "\n",
        "#Text-encoding: UTF-8 encoder\n",
        "def to_unicode(text):\n",
        "    if isinstance(text, float):\n",
        "        text = str(text)\n",
        "    if isinstance(text, int):\n",
        "        text = str(text)\n",
        "    if not isinstance(text, str):\n",
        "        text = text.decode('utf-8', 'ignore')\n",
        "    return text\n",
        "\n",
        "#Removing the square brackets\n",
        "def remove_between_square_brackets(text):\n",
        "    return re.sub('\\[[^]]*\\]', '', text)\n",
        "\n",
        "\n",
        "#Define function for removing special characters\n",
        "def remove_special_characters(text, remove_digits=True):\n",
        "    pattern=r'[^a-zA-z0-9\\s]'\n",
        "    text=re.sub(pattern,'',text)\n",
        "    return text\n",
        "\n",
        "#Removing the noisy text\n",
        "def denoise_text(text):\n",
        "    text = to_unicode(text)\n",
        "    text = strip_html(text)\n",
        "    text = re.sub(r\"http\\S+\", \"\", text)\n",
        "    text = deEmojify(text)\n",
        "    text = text.encode('ascii', 'ignore')\n",
        "    text = to_unicode(text)\n",
        "    text = remove_between_square_brackets(text)\n",
        "    text = remove_special_characters(text)\n",
        "    text = text.lower()\n",
        "    return text\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yo5d1-ark6E1"
      },
      "source": [
        "emails['text']=emails['text'].apply(denoise_text)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-aTeQllvlNt0"
      },
      "source": [
        "#Removing stopwords\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "\n",
        "#Tokenization of text\n",
        "tokenizer=ToktokTokenizer() #for every function\n",
        "\n",
        "#Setting English stopwords\n",
        "stopword_list=nltk.corpus.stopwords.words('english')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pDVPORwflmN0",
        "outputId": "a42b494d-ca24-4195-f641-16490f67f366"
      },
      "source": [
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "\n",
        "\n",
        "stop=set(stopwords.words('english'))\n",
        "print(stop)\n",
        "\n",
        "#Removing the stopwords\n",
        "def remove_stopwords(text, is_lower_case=False):\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    tokens = [token.strip() for token in tokens]\n",
        "    if is_lower_case:\n",
        "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
        "    else:\n",
        "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
        "    filtered_text = ' '.join(filtered_tokens)    \n",
        "    return filtered_text\n",
        "\n",
        "#Apply function on review column\n",
        "emails['text']=emails['text'].apply(remove_stopwords)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'just', 'doesn', 'she', 'about', 'again', 'them', 'until', 'then', \"mustn't\", \"wasn't\", 'when', 'i', 'himself', 'how', 's', \"isn't\", 'y', 'it', 'there', 'whom', 'mightn', 'hers', 'an', \"couldn't\", \"needn't\", 'down', 'while', \"hadn't\", 'has', 'with', \"hasn't\", 'but', 'all', 'don', 'aren', 'll', 'yourselves', 'into', 'mustn', 'be', 'through', 'no', 'its', 'after', 'their', 'which', 'my', \"should've\", 'a', \"shouldn't\", 'her', 'should', 'and', \"it's\", 'have', \"didn't\", 'theirs', 'we', 'on', 'here', \"don't\", 'such', 'each', 'needn', 'below', 'isn', 'some', 'out', 'what', 'from', 't', 'didn', 'more', 'to', 'can', 'is', 'am', 'above', 'before', 'by', 'too', \"shan't\", 'yourself', 'in', 'myself', 'off', 'hasn', 'ours', \"you'd\", 'because', 'his', 'once', 'very', 're', 've', 'they', 'where', 'd', \"doesn't\", 'ma', \"mightn't\", \"you'll\", 'm', \"weren't\", \"haven't\", 'o', 'as', 'if', 'any', 'against', 'couldn', 'shouldn', 'or', 'being', \"wouldn't\", 'further', \"aren't\", 'were', 'doing', 'herself', 'was', 'only', 'up', 'wouldn', 'both', 'having', 'me', 'you', 'themselves', 'will', 'does', 'the', \"that'll\", 'shan', 'so', 'not', 'did', 'do', \"you've\", 'few', 'had', 'him', 'wasn', 'ourselves', 'most', 'this', 'why', 'own', 'ain', 'for', 'that', 'under', 'yours', 'your', 'weren', 'are', 'won', 'at', 'over', \"you're\", \"won't\", 'other', 'itself', 'hadn', 'our', 'these', 'he', 'of', 'been', 'who', 'between', 'now', 'haven', \"she's\", 'nor', 'than', 'during', 'those', 'same'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pt1J_rNsmb8d",
        "outputId": "3e524ae6-518f-4e40-fd52-107617321bf3"
      },
      "source": [
        "#Stemming and Lemmatization\n",
        "from nltk.stem import WordNetLemmatizer,SnowballStemmer\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def simple_stemmer(text):\n",
        "    ps=SnowballStemmer(language='english')\n",
        "    return ' '.join([ps.stem(word) for word in tokenizer.tokenize(text)])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0LBAoninmw_c",
        "outputId": "a472609d-4790-4ff7-81b0-badb9ab27d25"
      },
      "source": [
        "from nltk.tag import pos_tag\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "#Lemmatizer example\n",
        "def lemmatize_all(sentence):\n",
        "    wnl = WordNetLemmatizer()\n",
        "    for word, tag in pos_tag(word_tokenize(sentence)):\n",
        "        if tag.startswith(\"NN\"):\n",
        "            yield wnl.lemmatize(word, pos='n')\n",
        "        elif tag.startswith('VB'):\n",
        "            yield wnl.lemmatize(word, pos='v')\n",
        "        elif tag.startswith('JJ'):\n",
        "            yield wnl.lemmatize(word, pos='a')\n",
        "        else:\n",
        "            yield word\n",
        "            \n",
        "def lemmatize_text(text):\n",
        "    return ' '.join(lemmatize_all(text))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKOx5cvKnEHO"
      },
      "source": [
        "emails['text']=emails['text'].apply(lemmatize_text)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vvFS6lSneaE"
      },
      "source": [
        "#Creating features using Bag of words model and building the Logistic Regression model\n",
        "#Transformed train reviews\n",
        "norm_text=emails.text"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ClzLv-LuobVs",
        "outputId": "ea5556a0-62f5-4669-99e8-f9dcdac59684"
      },
      "source": [
        "from sklearn.preprocessing import LabelBinarizer\n",
        "\n",
        "#Labelling the sentient data\n",
        "lb=LabelBinarizer()\n",
        "\n",
        "#Transformed sentiment data\n",
        "spam=lb.fit_transform(emails['spam'])\n",
        "print(spam.shape)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5728, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2Fd2oLqoqC0"
      },
      "source": [
        "#Binarisation of spam or ham: ham: 0, spam: 1"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "na2LlprGo5fL"
      },
      "source": [
        "#Fitting the bag of words model on the entire dataset\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "#Creating a matrix with reviews in row and unique words as columns and frequency of word in review as values.\n",
        "#Count vectorizer for bag of words\n",
        "cv=CountVectorizer()\n",
        "\n",
        "#Fitting model on entire data\n",
        "cv_fit = cv.fit(norm_text)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQ_bt0uBpK59",
        "outputId": "47149216-6c1e-4c42-f6d2-543f6620a059"
      },
      "source": [
        "#Normalised train reviews\n",
        "norm_train_text=emails.text[:3500]\n",
        "print('train:','\\n',norm_train_text[0])\n",
        "norm_train_cv_text=cv_fit.transform(norm_train_text)\n",
        "\n",
        "#Normalised test reviews\n",
        "norm_test_text=emails.text[3500:]\n",
        "print('test:','\\n',norm_test_text[3501])\n",
        "norm_test_cv_text=cv_fit.transform(norm_test_text)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: \n",
            " subject naturally irresistible corporate identity lt really hard recollect company market full suqgestions information isoverwhelminq good catchy logo stylish statlonery outstanding website make task much easy promise havinq order iogo company automaticaily become world ieader isguite ciear without good product effective business organization practicable aim hotat nowadays market promise marketing effort become much effective list clear benefit creativeness hand make original logo specially do reflect distinctive company image convenience logo stationery provide format easy use content management system letsyou change website content even structure promptness see logo draft within three business day affordability marketing break make gap budget 100 satisfaction guarantee provide unlimited amount change extra fee surethat love result collaboration look portfolio _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ interested _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
            "test: \n",
            " subject preface book julie introduction look fine make cosmetic change typos split infinitive slip safely ignore english even second language correction pink vince julie 08 01 2000 07 43 10 vincejkaminski cc subject preface book vince hope well spoke ago write preface book kindly offer would provide still possible realise extremely busy chris le go ahead write something want review change write preface would appreciate let know thought thanks julie get close preface one main objective write energy derivative price risk management bring together many various approach price risk management energy derivative possible discus depth model show relate way hope help reader analyse different model price wide range energy derivative build risk management system use consistent model framework believe practitioner last point important continue stress article presentation danger flaw risk management give arbitrage opportunity competitor use ad hoc inconsistent model different instrument market see also others propose consistent model however wish concentrate one particular model model exclusion others believe choice rest user although probably clear discussion model prefer therefore try give clear account possible advantage disadvantage model reader make informed choice model best suit need order meet objective book divide 11 chapter chapter 1 give overview fundamental principal need model price energy derivative underpin remainder book addition introduce technique underlie black scholes model framework outline numerical technique trinomial tree monte carlo simulation derivative pricing use throughout book chapter 2 discus analysis spot energy price well analyse empirical price movement propose number process use model price look well know process geometric brownian motion well mean reversion stochastic volatility jump process discuss show simulated parameter estimate chapter 3 write vince kaminski grant masson ronnie chahal enron corp discuss volatility estimation energy commodity market chapter build previous one examine detail method merit pitfall volatility estimation process assume different pricing model introduce chapter 2 example crude gas electricity market use illustrate technical interpretative aspect calculate volatility chapter 4 examines forward curve energy market although curve well understood straight forward financial market difficulty storage many energy market lead less well defined curve chapter describe forward price bound energy price build forward curve market instrument outline three main approach apply build forward curve energy market arbitrage approach econometric approach derive analytical value model underlying stochastic factor chapter 5 present overview structure find energy derivative market discuss us example product analyse chapter include variety swap cap floor collar well energy swaptions compound option asian option barrier option lookback option ladder option chapter 6 investigates single multi factor model energy spot price price standard energy derivative close form solution forward price forward volatility european option price spot forward derive presented model chapter include three factor stochastic convenience yield interest rate model chapter 7 show price path dependent american style option evaluate model chapter 6 simulation scheme develop evaluation european style option apply variety path dependent option order price option incorporate early exercise opportunity trinomial tree scheme develop tree build consistent observe forward curve use price exotic well standard european american style option chapter 8 describes methodology value energy option base model whole market observe forward curve approach result multi factor model able realistically capture evolution wide range energy forward curve user define volatility structure extremely general form close form solution develop price standard european option efficient monte carlo scheme present price exotic option chapter close discussion valuation american style option chapter 9 focus risk management energy derivative position chapter discus management price risk institution trade option derivative face problem manage risk time begin delta hedge portfolio contain derivative look extension gamma hedge illustrate technique use spot forward curve model general model present chapter 8 ideally suit multi factor hedge portfolio energy derivative also discuss chapter 10 examines key risk management concept value risk var apply portfolio contain energy derivative product discuss concept measure look key input volatility covariance correlation etc estimate compare fours major methodology compute var delta delta gamma historical simulation monte carlo simulation apply portfolio energy option chapter also look test var estimate various underlie energy market variable finally chapter 11 review model approach credit risk look detail two quite different approach creditmetrics j p morgan 1997 creditrisk credit suisse financial product 1997 detailed information publicly available together provide extensive set tool measure credit risk present numerical example apply technique energy derivative begin stress model method present book tool use benefit understand tool market work technique describe certainly magic wand 8 wave data risk management problem provide instant perfect solution quote riskmetrics technical document amount sophisticate analytics replace experience professional judgement manage risk 8 however right tool correctly use make job lot easy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMD4SNlxpc4v"
      },
      "source": [
        "spam_train=spam[:3500]\n",
        "spam_test=spam[3500:]"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aAzZFKHvqebt",
        "outputId": "9b634962-df4c-4dbe-9611-810f6141ea6c"
      },
      "source": [
        "emails.spam.value_counts()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    4360\n",
              "1    1368\n",
              "Name: spam, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PR7gA_cHqpTr"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KW0wtTYiq5TQ"
      },
      "source": [
        "n_t=norm_train_cv_text.shape[1]"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqEtY1ClrB_Z"
      },
      "source": [
        "model=Sequential()\n",
        "model.add(Dense(50,input_shape=(n_t,),activation='relu'))\n",
        "model.add(Dense(20,activation='relu'))\n",
        "#model.add(Dense(25,activation='relu'))\n",
        "\n",
        "model.add(Dense(1,activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HoqJNgerKNp",
        "outputId": "d9bbb0e1-8451-4391-9f00-9cf950b21217"
      },
      "source": [
        "history=model.fit(norm_train_cv_text,spam_train,epochs=10,verbose=2,validation_data=(norm_test_cv_text,spam_test))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "110/110 - 4s - loss: 0.1993 - accuracy: 0.9380 - val_loss: 0.0390 - val_accuracy: 0.9919\n",
            "Epoch 2/10\n",
            "110/110 - 1s - loss: 0.0218 - accuracy: 0.9989 - val_loss: 0.0293 - val_accuracy: 0.9937\n",
            "Epoch 3/10\n",
            "110/110 - 1s - loss: 0.0063 - accuracy: 1.0000 - val_loss: 0.0373 - val_accuracy: 0.9933\n",
            "Epoch 4/10\n",
            "110/110 - 1s - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.0538 - val_accuracy: 0.9906\n",
            "Epoch 5/10\n",
            "110/110 - 1s - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.0556 - val_accuracy: 0.9910\n",
            "Epoch 6/10\n",
            "110/110 - 1s - loss: 8.5239e-04 - accuracy: 1.0000 - val_loss: 0.0641 - val_accuracy: 0.9910\n",
            "Epoch 7/10\n",
            "110/110 - 1s - loss: 5.6315e-04 - accuracy: 1.0000 - val_loss: 0.0673 - val_accuracy: 0.9910\n",
            "Epoch 8/10\n",
            "110/110 - 1s - loss: 3.9543e-04 - accuracy: 1.0000 - val_loss: 0.0722 - val_accuracy: 0.9910\n",
            "Epoch 9/10\n",
            "110/110 - 1s - loss: 2.9018e-04 - accuracy: 1.0000 - val_loss: 0.0779 - val_accuracy: 0.9910\n",
            "Epoch 10/10\n",
            "110/110 - 1s - loss: 2.2007e-04 - accuracy: 1.0000 - val_loss: 0.0800 - val_accuracy: 0.9910\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P68CZGWwrWGj",
        "outputId": "4b2570c6-498b-42dc-ca7c-822d7b8395dc"
      },
      "source": [
        "loss,acc=model.evaluate(norm_test_cv_text,spam_test)\n",
        "print(\"acccuracy\",acc*100,\"%\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "70/70 [==============================] - 0s 5ms/step - loss: 0.0800 - accuracy: 0.9910\n",
            "acccuracy 99.10233616828918 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2VyVcuzrj7Q"
      },
      "source": [
        "#unseeen external data\n",
        "t={'text':[\"Dear Younus Ahmad Dar,Thank you for participating in the Psychometric Test - Gurgaon scheduled by WNS Global Services Private Limited . Your responses were successfully submitted on 15 Sep, 2021 11:49 AM .Best Regards,WNS Global Services Private Limited\"]}\n",
        "y=pd.DataFrame.from_dict(t,orient='columns')\n",
        "test=cv_fit.transform(y['text'])"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DiSMnKeKs25v",
        "outputId": "35c08a70-a668-4149-a95c-8e470181b8d0"
      },
      "source": [
        "model.predict(test[0])"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.59786606]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ECE4Rvlcs-Ur",
        "outputId": "b4ddc2fa-5c02-4375-ccfb-12c51c647991"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression,SGDClassifier\n",
        "\n",
        "#Training the model\n",
        "lr=LogisticRegression(penalty='l2',max_iter=500,C=1,random_state=42)\n",
        "\n",
        "#Fitting the model for the bag of words\n",
        "lr_bow=lr.fit(norm_train_cv_text,spam_train)\n",
        "print(lr_bow)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=500,\n",
            "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
            "                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69Xe3pFgvbho",
        "outputId": "a392df00-5d1d-498d-91a1-10b82c97a128"
      },
      "source": [
        "#Predicting the model for bag of words\n",
        "lr_bow_predict=lr.predict(norm_test_cv_text)\n",
        "print(lr_bow_predict)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 0 0 ... 0 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03TTZcQLjwAu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2V6yxz8jxd6"
      },
      "source": [
        "####unseen external data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0MsDX-mMvcj"
      },
      "source": [
        "#unseeen external data\n",
        "d={'text':[\"you will recive the bitcoins worth $3000 plz share  your bank details with us\"]}\n",
        "x=pd.DataFrame.from_dict(d,orient='columns')\n",
        "test_1=cv_fit.transform(x['text'])"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rUydBgjQLt0f",
        "outputId": "a37beeb1-77a6-4afa-bebf-6032b398dc8f"
      },
      "source": [
        "#Predicting the model for bag of words\n",
        "lr_bow_predict=lr.predict(test_1)\n",
        "print(lr_bow_predict)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "id": "NZFktgVpQ9RG",
        "outputId": "9b107a27-d627-48f2-c579-55fb156f4ab1"
      },
      "source": [
        "frames=[y,x]\n",
        "data=pd.concat(frames,axis=0,ignore_index=True)\n",
        "#data=data.reshape(1,)\n",
        "\n",
        "test_2=cv_fit.transform(data['text'])\n",
        "#data[0][0]\n",
        "data.head()"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Dear Younus Ahmad Dar,Thank you for participat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>you will recive the bitcoins worth $3000 plz s...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text\n",
              "0  Dear Younus Ahmad Dar,Thank you for participat...\n",
              "1  you will recive the bitcoins worth $3000 plz s..."
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmalzMBtTKyV"
      },
      "source": [
        "test_2.toarray().shape\n",
        "\n",
        "  #print(i[x])\n",
        "data_1=pd.DataFrame({\"data\":[data['text'][0],data['text'][1]]})\n",
        "  #x=x+1\n",
        "#data['text'][0]"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "id": "uAqV7R4NYk0v",
        "outputId": "e4e4663a-a3d7-49a6-9e90-6cc6c1fc382e"
      },
      "source": [
        "test_2.toarray().shape\n",
        "data_1.head()\n",
        "#data['text'][1]"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>data</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Dear Younus Ahmad Dar,Thank you for participat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>you will recive the bitcoins worth $3000 plz s...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                data\n",
              "0  Dear Younus Ahmad Dar,Thank you for participat...\n",
              "1  you will recive the bitcoins worth $3000 plz s..."
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r_vjucVGUpyH",
        "outputId": "ebe70e38-04f9-43f9-8ede-a079a5d3361f"
      },
      "source": [
        "#Predicting the model for bag of words\n",
        "#for i in data[0]:\n",
        "  #da=cv_fit.fit_transform(i)\n",
        "lr_bow_predict=lr.predict(test_2)\n",
        "print(lr_bow_predict)\n",
        "lr_bow_predict[0]\n",
        "lr_bow_predict[1]"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2db97_FV_B2"
      },
      "source": [
        "\n",
        "d={\"pred\":[lr_bow_predict[0],lr_bow_predict[1]]}\n",
        "#data_1.head()"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "id": "6Bbfe9IZU-JY",
        "outputId": "331ec2e0-f1ab-485f-c9fc-5f43035f066c"
      },
      "source": [
        "#lr_bow_predict.reshape(1,)\n",
        "prediction=pd.DataFrame.from_dict(d,orient=\"columns\",)\n",
        "prediction.head()"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pred</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   pred\n",
              "0     0\n",
              "1     1"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "id": "PYlCLVwCbl5T",
        "outputId": "9cc62d41-54cf-48ef-e390-f1cfe735411b"
      },
      "source": [
        "#fram=[data_1,prediction]\n",
        "pred_data=pd.concat([data_1,prediction],join='outer',axis=1)\n",
        "pred_data.head()"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>data</th>\n",
              "      <th>pred</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Dear Younus Ahmad Dar,Thank you for participat...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>you will recive the bitcoins worth $3000 plz s...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                data  pred\n",
              "0  Dear Younus Ahmad Dar,Thank you for participat...     0\n",
              "1  you will recive the bitcoins worth $3000 plz s...     1"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lRQFFIgywLKq",
        "outputId": "378f3811-7b7e-446e-c416-8928c5544764"
      },
      "source": [
        "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
        "\n",
        "#Accuracy score for bag of words\n",
        "lr_bow_score=accuracy_score(spam_test,lr_bow_predict)\n",
        "print(\"lr_bow_score :\",lr_bow_score)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lr_bow_score : 0.9874326750448833\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dt9qMAILwYJv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}